{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lesson1: Convolutional Neural Networks with dogscats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's classify images using deep learning and submit the result to Kaggle!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=5, micro=3, releaselevel='final', serial=0)\n",
      "0.9.0\n",
      "2.0.4\n"
     ]
    }
   ],
   "source": [
    "# make some Python3 functions available on Python2\n",
    "from __future__ import division, print_function\n",
    "\n",
    "import sys\n",
    "print(sys.version_info)\n",
    "\n",
    "import theano\n",
    "print(theano.__version__)\n",
    "\n",
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes Keras with Theano backend.\n",
    "- TODO: make TensorFlow version as another notebook\n",
    "\n",
    "It also assumes that you will run it on either one of these two cases:\n",
    "- Floydhub (--env theano:py2 -> Theano rel-0.8.2 + Keras 1.2.2 on Python2)\n",
    "- local conda virtual environment (Theano 0.9.0 + Keras 2.0.4 on Python3)\n",
    "\n",
    "Refer to [this FloydHub document](http://docs.floydhub.com/home/environments/) for available FloydHub environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to have these files in the parent directory of the directory where you execute this notebook.\n",
    "\n",
    "- available [in the official repo](https://github.com/fastai/courses/tree/master/deeplearning1/nbs) for Keras1 on Python2 (rename from original files)\n",
    "    - utils_keras1.py\n",
    "    - vgg16_keras1.py\n",
    "    - vgg16bn_keras1.py\n",
    "- available [in the unofficial repo](https://github.com/roebius/deeplearning1_keras2/tree/master/nbs) for Keras2 on Python3\n",
    "    - utils.py\n",
    "    - vgg16.py\n",
    "    - vgg16bn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directory structure looks like this. Please modifiy the symlinks according to your environment.\n",
    "\n",
    "- (*) only for FloydHub\n",
    "- (**) only for local\n",
    "\n",
    "```\n",
    "floyd_requirements.txt (*)\n",
    "floydhub.data.unzip/   (*)\n",
    "    (TODO)\n",
    "floydhub.data.zipped/  (*)\n",
    "    (TODO)\n",
    "lesson1/\n",
    "    data/ (**)\n",
    "        redux/\n",
    "            train/\n",
    "                cat.437.jpg\n",
    "                dog.9924.jpg\n",
    "                ...\n",
    "            test/\n",
    "                231.jpg\n",
    "                325.jpg\n",
    "                ...\n",
    "    dogscats_run.ipynb\n",
    "    floyd_requirements.txt -> ../floyd_requirements.txt (*)\n",
    "    utils.py -> ../utils(_keras1).py\n",
    "    vgg16.py -> ../vgg16(_keras1).py\n",
    "    vgg16bn.py -> ../vgg16bn(_keras1).py\n",
    "utils.py\n",
    "utils_keras1.py\n",
    "vgg16.py\n",
    "vgg16_keras1.py\n",
    "vgg16bn.py\n",
    "vgg16bn_keras1.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36m__pycache__\u001b[m\u001b[m/            submission1.csv         \u001b[35mvgg16bn.py\u001b[m\u001b[m@\n",
      "dogscats_run.ipynb      \u001b[35mutils.py\u001b[m\u001b[m@\n",
      "\u001b[35mfloyd_requirements.txt\u001b[m\u001b[m@ \u001b[35mvgg16.py\u001b[m\u001b[m@\n",
      "bcolz\n",
      "from __future__ import division,print_function\n",
      "import math, os, json, sys, re\n",
      "\n",
      "# import cPickle as pickle  # Python 2\n",
      "import pickle  # Python3\n",
      "\n",
      "from glob import glob\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "from operator import itemgetter, attrgetter, methodcaller\n",
      "from collections import OrderedDict\n",
      "import itertools\n",
      "from itertools import chain\n",
      "\n",
      "import pandas as pd\n",
      "import PIL\n",
      "from PIL import Image\n",
      "from numpy.random import random, permutation, randn, normal, uniform, choice\n",
      "from numpy import newaxis\n",
      "import scipy\n",
      "from scipy import misc, ndimage\n",
      "from scipy.ndimage.interpolation import zoom\n",
      "from scipy.ndimage import imread\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import bcolz\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.manifold import TSNE\n",
      "\n",
      "from IPython.lib.display import FileLink\n",
      "\n",
      "import theano\n",
      "from theano import shared, tensor as T\n",
      "from theano.tensor.nnet import conv2d, nnet\n",
      "from theano.tensor.signal import pool\n",
      "\n",
      "import keras\n",
      "from keras import backend as K\n",
      "from keras.utils.data_utils import get_file\n",
      "from keras.utils import np_utils\n",
      "from keras.utils.np_utils import to_categorical\n",
      "from keras.models import Sequential, Model\n",
      "from keras.layers import Input, Embedding, Reshape, merge, LSTM, Bidirectional\n",
      "from keras.layers import SpatialDropout1D, Concatenate  # Keras2\n",
      "\n",
      "from keras.layers import TimeDistributed, Activation, SimpleRNN, GRU\n",
      "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
      "\n",
      "# from keras.regularizers import l2, activity_l2, l1, activity_l1  # Keras1\n",
      "from keras.regularizers import l2, l1  # Keras2\n",
      "\n",
      "from keras.layers.normalization import BatchNormalization\n",
      "from keras.optimizers import SGD, RMSprop, Adam\n",
      "\n",
      "# from keras.utils.layer_utils import layer_from_config  # Keras1\n",
      "from keras.layers import deserialize  # Keras 2\n",
      "from keras.layers.merge import dot, add, concatenate  # Keras2\n",
      "from keras.metrics import categorical_crossentropy, categorical_accuracy\n",
      "from keras.layers.convolutional import *\n",
      "from keras.preprocessing import image, sequence\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "\n",
      "from vgg16 import *\n",
      "from vgg16bn import *\n",
      "np.set_printoptions(precision=4, linewidth=100)\n",
      "\n",
      "\n",
      "to_bw = np.array([0.299, 0.587, 0.114])\n",
      "\n",
      "def gray(img):\n",
      "    if K.image_dim_ordering() == 'tf':\n",
      "        return np.rollaxis(img, 0, 1).dot(to_bw)\n",
      "    else:\n",
      "        return np.rollaxis(img, 0, 3).dot(to_bw)\n",
      "\n",
      "def to_plot(img):\n",
      "    if K.image_dim_ordering() == 'tf':\n",
      "        return np.rollaxis(img, 0, 1).astype(np.uint8)\n",
      "    else:\n",
      "        return np.rollaxis(img, 0, 3).astype(np.uint8)\n",
      "\n",
      "def plot(img):\n",
      "    plt.imshow(to_plot(img))\n",
      "\n",
      "\n",
      "def floor(x):\n",
      "    return int(math.floor(x))\n",
      "def ceil(x):\n",
      "    return int(math.ceil(x))\n",
      "\n",
      "def plots(ims, figsize=(12,6), rows=1, interp=False, titles=None):\n",
      "    if type(ims[0]) is np.ndarray:\n",
      "        ims = np.array(ims).astype(np.uint8)\n",
      "        if (ims.shape[-1] != 3):\n",
      "            ims = ims.transpose((0,2,3,1))\n",
      "    f = plt.figure(figsize=figsize)\n",
      "    for i in range(len(ims)):\n",
      "        sp = f.add_subplot(rows, len(ims)//rows, i+1)\n",
      "        sp.axis('Off')\n",
      "        if titles is not None:\n",
      "            sp.set_title(titles[i], fontsize=16)\n",
      "        plt.imshow(ims[i], interpolation=None if interp else 'none')\n",
      "\n",
      "\n",
      "def do_clip(arr, mx):\n",
      "    clipped = np.clip(arr, (1-mx)/1, mx)\n",
      "    return clipped/clipped.sum(axis=1)[:, np.newaxis]\n",
      "\n",
      "\n",
      "def get_batches(dirname, gen=image.ImageDataGenerator(), shuffle=True, batch_size=4, class_mode='categorical',\n",
      "                target_size=(224,224)):\n",
      "    return gen.flow_from_directory(dirname, target_size=target_size,\n",
      "            class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)\n",
      "\n",
      "\n",
      "def onehot(x):\n",
      "    return to_categorical(x)\n",
      "\n",
      "\n",
      "def wrap_config(layer):\n",
      "    return {'class_name': layer.__class__.__name__, 'config': layer.get_config()}\n",
      "\n",
      "\n",
      "def copy_layer(layer): return deserialize(wrap_config(layer))  # Keras2\n",
      "\n",
      "\n",
      "def copy_layers(layers): return [copy_layer(layer) for layer in layers]\n",
      "\n",
      "\n",
      "def copy_weights(from_layers, to_layers):\n",
      "    for from_layer,to_layer in zip(from_layers, to_layers):\n",
      "        to_layer.set_weights(from_layer.get_weights())\n",
      "\n",
      "\n",
      "def copy_model(m):\n",
      "    res = Sequential(copy_layers(m.layers))\n",
      "    copy_weights(m.layers, res.layers)\n",
      "    return res\n",
      "\n",
      "\n",
      "def insert_layer(model, new_layer, index):\n",
      "    res = Sequential()\n",
      "    for i,layer in enumerate(model.layers):\n",
      "        if i==index: res.add(new_layer)\n",
      "        copied = deserialize(wrap_config(layer))  # Keras2\n",
      "        res.add(copied)\n",
      "        copied.set_weights(layer.get_weights())\n",
      "    return res\n",
      "\n",
      "\n",
      "def adjust_dropout(weights, prev_p, new_p):\n",
      "    scal = (1-prev_p)/(1-new_p)\n",
      "    return [o*scal for o in weights]\n",
      "\n",
      "\n",
      "def get_data(path, target_size=(224,224)):\n",
      "    batches = get_batches(path, shuffle=False, batch_size=1, class_mode=None, target_size=target_size)\n",
      "    return np.concatenate([batches.next() for i in range(batches.samples)])  # Keras2\n",
      "\n",
      "\n",
      "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
      "    \"\"\"\n",
      "    This function prints and plots the confusion matrix.\n",
      "    Normalization can be applied by setting `normalize=True`.\n",
      "    (This function is copied from the scikit docs.)\n",
      "    \"\"\"\n",
      "    plt.figure()\n",
      "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
      "    plt.title(title)\n",
      "    plt.colorbar()\n",
      "    tick_marks = np.arange(len(classes))\n",
      "    plt.xticks(tick_marks, classes, rotation=45)\n",
      "    plt.yticks(tick_marks, classes)\n",
      "\n",
      "    if normalize:\n",
      "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
      "    print(cm)\n",
      "    thresh = cm.max() / 2.\n",
      "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
      "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
      "\n",
      "    plt.tight_layout()\n",
      "    plt.ylabel('True label')\n",
      "    plt.xlabel('Predicted label')\n",
      "\n",
      "\n",
      "def save_array(fname, arr):\n",
      "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
      "    c.flush()\n",
      "\n",
      "\n",
      "def load_array(fname):\n",
      "    return bcolz.open(fname)[:]\n",
      "\n",
      "\n",
      "def mk_size(img, r2c):\n",
      "    r,c,_ = img.shape\n",
      "    curr_r2c = r/c\n",
      "    new_r, new_c = r,c\n",
      "    if r2c>curr_r2c:\n",
      "        new_r = floor(c*r2c)\n",
      "    else:\n",
      "        new_c = floor(r/r2c)\n",
      "    arr = np.zeros((new_r, new_c, 3), dtype=np.float32)\n",
      "    r2=(new_r-r)//2\n",
      "    c2=(new_c-c)//2\n",
      "    arr[floor(r2):floor(r2)+r,floor(c2):floor(c2)+c] = img\n",
      "    return arr\n",
      "\n",
      "\n",
      "def mk_square(img):\n",
      "    x,y,_ = img.shape\n",
      "    maxs = max(img.shape[:2])\n",
      "    y2=(maxs-y)//2\n",
      "    x2=(maxs-x)//2\n",
      "    arr = np.zeros((maxs,maxs,3), dtype=np.float32)\n",
      "    arr[floor(x2):floor(x2)+x,floor(y2):floor(y2)+y] = img\n",
      "    return arr\n",
      "\n",
      "\n",
      "def vgg_ft(out_dim):\n",
      "    vgg = Vgg16()\n",
      "    vgg.ft(out_dim)\n",
      "    model = vgg.model\n",
      "    return model\n",
      "\n",
      "def vgg_ft_bn(out_dim):\n",
      "    vgg = Vgg16BN()\n",
      "    vgg.ft(out_dim)\n",
      "    model = vgg.model\n",
      "    return model\n",
      "\n",
      "\n",
      "def get_classes(path):\n",
      "    batches = get_batches(path+'train', shuffle=False, batch_size=1)\n",
      "    val_batches = get_batches(path+'valid', shuffle=False, batch_size=1)\n",
      "    test_batches = get_batches(path+'test', shuffle=False, batch_size=1)\n",
      "    return (val_batches.classes, batches.classes, onehot(val_batches.classes), onehot(batches.classes),\n",
      "        val_batches.filenames, batches.filenames, test_batches.filenames)\n",
      "\n",
      "\n",
      "def split_at(model, layer_type):\n",
      "    layers = model.layers\n",
      "    layer_idx = [index for index,layer in enumerate(layers)\n",
      "                 if type(layer) is layer_type][-1]\n",
      "    return layers[:layer_idx+1], layers[layer_idx+1:]\n",
      "\n",
      "\n",
      "class MixIterator(object):\n",
      "    def __init__(self, iters):\n",
      "        self.iters = iters\n",
      "        self.multi = type(iters) is list\n",
      "        if self.multi:\n",
      "            self.N = sum([it[0].N for it in self.iters])\n",
      "        else:\n",
      "            self.N = sum([it.N for it in self.iters])\n",
      "\n",
      "    def reset(self):\n",
      "        for it in self.iters: it.reset()\n",
      "\n",
      "    def __iter__(self):\n",
      "        return self\n",
      "\n",
      "    def next(self, *args, **kwargs):\n",
      "        if self.multi:\n",
      "            nexts = [[next(it) for it in o] for o in self.iters]\n",
      "            n0 = np.concatenate([n[0] for n in nexts])\n",
      "            n1 = np.concatenate([n[1] for n in nexts])\n",
      "            return (n0, n1)\n",
      "        else:\n",
      "            nexts = [next(it) for it in self.iters]\n",
      "            n0 = np.concatenate([n[0] for n in nexts])\n",
      "            n1 = np.concatenate([n[1] for n in nexts])\n",
      "            return (n0, n1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check current directory\n",
    "%pwd\n",
    "%ls\n",
    "\n",
    "# see some files are loaded fine\n",
    "%cat floyd_requirements.txt\n",
    "\n",
    "# check no Keras2 specific function is used (when Keras1 is used)\n",
    "%cat utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create references to important directories we will use over and over\n",
    "import os, sys\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "LESSON_HOME_DIR = current_dir\n",
    "\n",
    "# FloydHub\n",
    "DATA_HOME_DIR = \"/input/dogscats/\"\n",
    "OUTPUT_HOME_DIR = \"/output/\"\n",
    "\n",
    "# local\n",
    "#DATA_HOME_DIR = current_dir+'/data/redux'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import modules\n",
    "from utils import *\n",
    "from vgg16 import Vgg16\n",
    "\n",
    "#Instantiate plotting tool\n",
    "#In Jupyter notebooks, you will need to run this command before doing any plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(omit) prepared in floydhub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FloydHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below shows how to update data to FloydHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd ../floydhub.data.zipped/; pwd\n",
    "\n",
    "# empty\n",
    "ls -l\n",
    "\n",
    "wget http://files.fast.ai/files/dogscats.zip\n",
    "\n",
    "# upload the zipped dataset to floydnet, and create a floydnet dataset\n",
    "floyd data init dogscats.zipped\n",
    "floyd data upload\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: /input/dogscats/: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# FloydHub: check data\n",
    "%ls /input/dogscats/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/kazuhirokomoda/deep_learning/blob/master/fast.ai/17flowers_data.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/input/dogscats/'\n",
      "/Users/kkomoda/git/deep_learning/fast.ai/lesson1\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR\n",
    "\n",
    "#Set path to sample/ path if desired\n",
    "path = DATA_HOME_DIR + '/' #'/sample/'\n",
    "test_path = DATA_HOME_DIR + '/test1/' #We use all the test data\n",
    "\n",
    "# in FloydHub, data needs to be output under /output\n",
    "# if results_path cannot be created, execute mkdir directly in the terminal\n",
    "results_path = OUTPUT_HOME_DIR + '/results/'\n",
    "%mkdir results_path\n",
    "\n",
    "train_path = path + '/train/'\n",
    "valid_path = path + '/valid/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a pretrained VGG model with our Vgg16 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As large as you can, but no larger than 64 is recommended.\n",
    "#batch_size = 8\n",
    "batch_size = 64\n",
    "\n",
    "no_of_epochs=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original pre-trained Vgg16 class classifies images into one of the 1000 categories. This number of categories depends on the dataset which Vgg16 was trained with. (http://image-net.org/challenges/LSVRC/2014/browse-synsets)\n",
    "\n",
    "In order to classify images into the categories which we prepare (2 categories of dogs/cats, in this notebook), *fine-tuning* technology is useful. It:\n",
    "- keeps the most weights from the pre-trained Vgg16 model, but modifies only a few parts of the weights\n",
    "- changes the dimension of the output layer (from 1000 to 2, in this notebook) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg = Vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grab a few images at a time for training and validation.\n",
    "batches = vgg.get_batches(train_path, batch_size=batch_size)\n",
    "val_batches = vgg.get_batches(valid_path, batch_size=batch_size*2)\n",
    "\n",
    "# Finetune: note that the vgg model is compiled inside the finetune method.\n",
    "vgg.finetune(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1020 images belonging to 17 classes.\n",
      "Found 170 images belonging to 17 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "128/128 [==============================] - 670s - loss: 2.0221 - acc: 0.5420 - val_loss: 0.6882 - val_acc: 0.7765\n"
     ]
    }
   ],
   "source": [
    "# Fit: note that we are passing in the validation dataset to the fit() method\n",
    "# For each epoch we test our model against the validation set\n",
    "latest_weights_filename = None\n",
    "for epoch in range(no_of_epochs):\n",
    "    print(\"Running epoch: %d\" % epoch)\n",
    "    vgg.fit(batches, val_batches, batch_size, nb_epoch=1)\n",
    "    latest_weights_filename = 'ft%d.h5' % epoch\n",
    "    vgg.model.save_weights(results_path+latest_weights_filename)\n",
    "print(\"Completed %s fit operations\" % no_of_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches, preds = vgg.test(test_path, batch_size = batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(preds[:5])\n",
    "\n",
    "filenames = batches.filenames\n",
    "print(filenames[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can verify the column ordering by viewing some images\n",
    "from PIL import Image\n",
    "Image.open(test_path + filenames[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save our test results arrays so we can use them again later\n",
    "save_array(results_path + 'test_preds.dat', preds)\n",
    "save_array(results_path + 'filenames.dat', filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate predictions on validation set, so we can find correct and incorrect examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.model.load_weights(results_path+latest_weights_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_batches, probs = vgg.test(valid_path, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = val_batches.filenames\n",
    "expected_labels = val_batches.classes #0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Round our predictions to 0/1 to generate labels\n",
    "our_predictions = probs[:,0]\n",
    "our_labels = np.round(1-our_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(TODO) look at data to improve model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(expected_labels, our_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm, val_batches.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Predictions to Kaggle!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load our test predictions from file\n",
    "preds = load_array(results_path + 'test_preds.dat')\n",
    "filenames = load_array(results_path + 'filenames.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Grab the dog prediction column\n",
    "isdog = preds[:,1]\n",
    "print(\"Raw Predictions: \" + str(isdog[:5]))\n",
    "print(\"Mid Predictions: \" + str(isdog[(isdog < .6) & (isdog > .4)]))\n",
    "print(\"Edge Predictions: \" + str(isdog[(isdog == 1) | (isdog == 0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sneaky trick to round down our edge predictions\n",
    "# Swap all ones with .95 and all zeros with .05\n",
    "isdog = isdog.clip(min=0.05, max=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract imageIds from the filenames in our test/unknown directory \n",
    "filenames = batches.filenames\n",
    "ids = np.array([int(f[8:f.find('.')]) for f in filenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = np.stack([ids,isdog], axis=1)\n",
    "subm[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd $DATA_HOME_DIR\n",
    "submission_file_name = 'submission1.csv'\n",
    "np.savetxt(submission_file_name, subm, fmt='%d,%.5f', header='id,label', comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "%cd $LESSON_HOME_DIR\n",
    "FileLink('data/redux/'+submission_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py3tf)",
   "language": "python",
   "name": "conda_py3tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
